# 第4章-朴素贝叶斯法-后验概率最大化{docsify-ignore-all}

&emsp;&emsp;在朴素贝叶斯法这一章中，书中有一个结论：

> 朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化。

## 推导
&emsp;&emsp;首先已知损失函数$L(Y, f(X))=\left\{\begin{array}{ll}{1,} & {Y \neq f(X)} \\ {0,} & {Y = f(X)}\end{array}\right.$，这里的$f(X)$对应于预测值，$Y$对应于真实值，需要计算的是$f(x)=\mathop{\arg\max}\limits_{c_k} P(Y=c_k|X)$，本来$f(X)$是决策函数的形式，$P(Y=c_k|X)$是条件概率分布的形式，通过这样的一个等式，将两种形式联系在一起。在什么情况下，满足上式呢？当最小化期望风险时，能够满足。  
期望风险为$E[L(Y,f(X))]$  
$\because Y$和$f(X)$都是离散的  
$ \begin{aligned} \therefore E[L(Y,f(X))]
&= \sum_Y \sum_X [L(Y,f(X))P(X,Y)] \\
&= \sum_Y \sum_X [L(Y,f(X))P(X)P(Y|X)] \\
&= \sum_X \left[[\sum_Y L(Y,f(X))P(Y|X)]P(X)\right]
\end{aligned}$  
要最小化$E[L(Y,f(X))]$就需要最小化每一个$X$的$\sum_Y L(Y,f(X))P(Y|X)$一项。  
$\therefore \min E[L(Y,f(X))] \Rightarrow \min \{\sum_Y L(Y,f(X))P(Y|X)\}$  
$\displaystyle \therefore \sum_{c_k} L(Y=c_k, f(x))P(Y=c_k|X)$  
中间这项$L(Y=c_k, f(x))$是损失函数，只有当$f(x) \neq c_k$时，取值为1  
$\begin{aligned} \therefore \min \{\sum_Y L(Y,f(X))P(Y|X)\}
&= \min \sum_{c_k}\Big[ I(f(x) \neq c_k)P(Y=c_k|X)\Big] \\
&= \min \sum_{c_k}\Big[ [1 - I(f(x)=c_k)]P(Y=c_k|X)\Big] \\
&= \min \Big\{\sum_{c_k} P(Y=c_k|X) - \sum_{c_k} \Big[I(f(x)=c_k)P(Y=c_k|X)\Big]\Big\}
\end{aligned}$  
$\displaystyle \because \sum_{c_k} P(Y=c_k|X) = 1$  
$ \begin{aligned} \therefore \min \Big\{\sum_{c_k} P(Y=c_k|X) - \sum_{c_k} \Big[I(f(x)=c_k)P(Y=c_k|X)\Big]\Big\}
&= \min \Big\{ 1 - \sum_{c_k} \Big[I(f(x)=c_k)P(Y=c_k|X)\Big] \Big\}\\ 
&= \max \sum_{c_k} \Big[I(f(x)=c_k)P(Y=c_k|X)\Big]
\end{aligned}$ 

## 解释 
&emsp;&emsp;对于每一个随机变量$X$，都有一个概率分布$Y$，$X \in \{c_1, c_2, \cdots, c_K\}, y \in \{p_1, p_2, \cdots, p_K\}$，对于$f(x)$而言，只能取其中的一个，假设$f(x)=c_1$时，那么输出$Y$的真实值只以概率$p_1$的可能性是$c_1$，只以概率$p_2$的可能性是$c_2$，...，只以概率$p_K$的可能性是$c_K$，当输出的真实值是$c_1$时，就预测正确，当出现其他类别时，预测错误。  
&emsp;&emsp;所以当用$f(x)$预测$Y$时，$Y$以一定的概率判定为其中的一个类别$c_k$，对于$c_1,\cdots,c_K$而言，只有其中的一个$k$使得$I(f(x)=c_k)=1$，因为$f(x)$只能取其中的一个值。  
&emsp;&emsp;所以要取最大值，取其中的一项使得$I(f(x)=c_k)=1$，要使得其最大，找一个$c_k$使得$P(Y=c_k|X)$最大。  
$\therefore f(x)=\mathop{\arg \max}\limits_{c_k} P(Y=c_k|X)$  
**后验概率最大化得证。** 