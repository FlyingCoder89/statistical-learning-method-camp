# 第4章-朴素贝叶斯法-导读{docsify-ignore-all}
&emsp;&emsp;朴素贝叶斯法也是用来做分类问题的，其对应的输出变量为$y \in \{c_1, c_2, \cdots, c_K\}$，输入变量为$x_i$（某些离散值），可能的取值个数为$S_i$。  
&emsp;&emsp;在介绍朴素贝叶斯法之前，先回顾一下第1章，在该章中，把统计学习方法进行了分类，根据模型形式的不同，可以分成决策函数和条件概率分布，在决策函数中，给定一个输入$X$，通过一个函数$f$，得到预测值$Y$，即$Y=f(X)$。在条件概率分布中，给定一个输入$X$，得到输出$Y$的条件概率分布$P(Y|X)$，通过该分布，对$Y$进行决策，如果$Y$是分类变量时，选取概率最大时所对应的分类。  
&emsp;&emsp;统计学习方法也可以分成生成模型和判别模型。  

- 生成模型：$\displaystyle P(Y|X)=\frac{P(X,Y)}{P(X)}$
- 判别模型：$Y=f(X),P(Y|X)$   

## 三个分类模型的比较
$$Y=f(x) \Rightarrow P(Y|X) \Rightarrow P(Y|X)=\frac{P(X, Y)}{P(X)}$$
1. 决策函数，首先不考虑$X和Y$的随机性；$Y$的条件概率分布，只考虑了$Y$的随机性，给定$X$时，$Y$有一个概率分布；第3种形式，同时考虑了$X和Y$的随机性，不仅有$Y$的条件分布、$X$的边际分布，还有$X和Y$的联合分布。所以从左到右，考虑的随机性是越来越多的。
2. 第2章-感知机模型，首先该模型属于决策函数的形式。第4章-朴素贝叶斯法，该模型直接从第1种形式到第3种形式，里面用到的最重要的公式是贝叶斯公式。

## 朴素贝叶斯法的学习与分类
&emsp;&emsp;首先举一个高考升学的例子，考查一个高中生能否考上985大学，其结果只有两种：$Y=\{c_1=考上,c_2=没考上\}$（$K$），有如下特征向量：生源地($X^{(1)}$)，是否来自重点中学($X^{(2)}$)。可得$P(X|Y=c_1)和P(X|Y=c_2)$两个不同的分布，即给定$Y$的取值，$X$的分布是不同的。  
&emsp;&emsp;已知以上信息，一个考生，考上和没考上的分布是什么？这个就是$Y$的分布，其分布为$P(Y=c_1)和P(Y=c_2)$。但如果获取信息增加，例如该考生来自北京，此时$Y$的分布如何（即求$P(Y|X^{(1)}=北京)$）？又该考生来自重点中学，此时的分布是什么（即求$P(Y|X^{(1)}=北京,X^{(2)}=重点中学)$）？在贝叶斯法中，目的是通过$P(X|Y=c_1)和P(X|Y=c_2)$得到$P(Y|X^{(1)}=北京,X^{(2)}=重点中学)$  

> 生成模型：$\displaystyle P(Y=c_k | X=x)=\frac{P(Y=c_k) \cdot P(X=x | Y=c_k)}{P(X=x)}$  
模型假设，条件独立性：$\displaystyle P(x=x | Y=c_{k})=\prod_{i=1}^n P(X^{(i)}=x^{(i)} | Y=c_k)$  
预测准则，后验概率最大：$y= \mathop{\arg \max} \limits_{c_k} P(Y=c_k | X=x)$  

根据上述算法，继续之前的举例：  
先求考上的先验概率为$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)}|Y=c_1)$  
$\because$ 在朴素贝叶斯法中，假设各分量之前是独立的。  
$\therefore P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)}|Y=c_1)=P(X^{(1)}=x^{(1)}|Y=c_1) \cdot P(X^{(2)}=x^{(2)}|Y=c_1)$  
根据生成模型，由于$P(X=x)$的值不变  
$\therefore$ 只需要对$P(Y=c_{k} | X=x)$求取最大值  

## 朴素贝叶斯法的参数估计
&emsp;&emsp;用训练集实例估计$P(Y=c_k), P(X=x|Y=c_k)$，有两种方法：  
### 极大似然估计
首先求出$\displaystyle P(Y=c_1)=\frac{\#\{y_i=c_1\}}{N}$，#表示求个数，N表示整个训练集实例总数。  
同样求得$\displaystyle P(Y=c_{2})=\frac{\#\{y_{i}=c_{2}\}}{N}$
现求出$\displaystyle P(X^{(1)}=x^{(1)}|Y=c_1) = \frac{\#\{y_i=c_1,X^{(1)}=x^{(1)}\}}{\#\{y_i=c_1\}}$  
将上述值代入生成模型贝叶斯公式中，即可求出$P(Y=c_{k}|X=x)$  
&emsp;&emsp;在极大似然估计中，可能会出现一种尴尬的情况，在考上的同学中，来自某一个地区的人数可能为0，此时分母$\#\{y_i=c_1\}=0$。出现这种情况的可能是当训练集中的实例比较少，分类类别比较多，就可能出现某一种类别没有对应的实例。所以这个时候，极大似然估计的效果会比较差，可换成贝叶斯估计。

### 贝叶斯估计
以下用估计$Y$的概率分布来举例：  
&emsp;&emsp;为了避免乘积为0，在求取$P(Y=c_1)$时，在分子处加一项$\lambda$，同时在分母处也加上一项$K \lambda$，即为$\displaystyle  P(Y=c_1)=\frac{\#\{y_i=c_1\} +\lambda}{N+K \lambda},P(Y=c_2)=\frac{\#\{y_i=c_2\} +\lambda}{N+K \lambda},\cdots,P(Y=c_K)=\frac{\#\{y_i=c_K\} +\lambda}{N+K \lambda}$  
所以$\displaystyle P(X^{(1)}=x^{(1)}|Y=c_1) = \frac{\#\left\{y_i=c_1,X^{(1)}=x^{(1)}\right\}+\lambda}{\#\{y_i=c_1\}+S_i \lambda}$  
再讨论一下先验概率和后验概率：  
&emsp;&emsp;在分类类别$Y$的分布，其中每一个类别的概率是$P(Y=c_i)=\theta_i$，所以$\sum \theta_i = 1$，需要估计的是$\theta_1,\theta_2,\cdots,\theta_K$的先验分布，该分布叫做Dirichlet（狄利克雷）分布，它是$\beta$分布在多维上的一个推广，公式是$\displaystyle \pi(\theta)=\frac{\Gamma(K \alpha)}{\Gamma(\alpha)^K} \theta_{1}^{\alpha-1} \theta_{2}^{\alpha-1} \cdots \theta_{k}^{\alpha-1}$，这就是$\theta$的分布——Dirichlet分布。  
&emsp;&emsp;再加上样本信息$y_i$出现的 ，根据贝叶斯公式，求$\theta$的后验分布，根据最大后验这个准则，就得到$\theta_1,\theta_2,\cdots,\theta_K$的贝叶斯估计。这里面出现的$\lambda和\alpha$是相对应的关系，并不是完全相等的关系。