# 第4章-朴素贝叶斯法-贝叶斯估计{docsify-ignore-all}

&emsp;&emsp;本章中对$Y$的分布的估计，以及$Y$的条件下$X$分布的估计，用到了两种估计方法：极大似然估计和贝叶斯估计。本节通过对$Y$分布的估计来介绍一下极大似然估计和贝叶斯估计得到的结果。  

## 问题描述
&emsp;&emsp;随机变量$Y$的分布是离散的，可能的取值为$c_1,c_2,\cdots,c_K$，$Y$属于多项分布（对于每一个类别，$Y$的取值都有一个对应的概率$\theta_i$），可知$\sum \theta_i=1$。  
  
&emsp;&emsp;下面用掷硬币的例子，$Y$出现的结果是正面或反面，正面出现的概率记为$\theta_1$，反面出现的概率记为$\theta_2$，$\theta_1+\theta_2=1$，如果只有两个结果，$Y$为二项分布，如果有$K$个结果，叫做多项分布。将K个$\theta$写在一起，记为向量$\hat{\theta}$。  
$\therefore Y$的分布为$P(Y=y|\theta)=\theta_1^{I\{y=c_1\}} \cdot \theta_2^{I\{y=c_2\}} \cdots \theta_K^{I\{y=c_K\}}$  

## 极大似然估计
&emsp;&emsp;在极大似然估计中，得到了$y_1,y_2,\cdots,y_N$样本，可得到联合概率分布：其中$m_i$表示出现结果是$c_i$的次数
$$\begin{aligned} 
P(y_1,y_2,\cdots,y_N | \theta) 
&= P(y_1|\theta)P(y_2|\theta) \cdots P(y_N|\theta) \\
&= \theta_1^{m_1}\theta_2^{m_2} \cdots \theta_N^{m_N} \\
\end{aligned}$$
极大似然估计的思想是通过最大化这个联合概率分布，寻找$\theta$的估计值：
$$\max P(y_1,y_2,\cdots,y_N | \theta) \Rightarrow \max lnP(\cdot) = m_1 \ln \theta_1 + m_2 \ln \theta_2 + \cdots + m_K \ln \theta_K$$
$\because \theta_1+\theta_2+\cdots+\theta_K=1$  
为求上式，需要引入拉格朗日乘子：$\displaystyle \max_{\theta_1,\cdots,\theta_K} m_1 \ln \theta_1 + m_2 \ln \theta_2 + \cdots + m_K \ln \theta_K + \lambda(\theta_1+\cdots+\theta_K-1)$  
对上式求偏导可得：  
$\begin{array}{c} 
\displaystyle \frac{m_1}{\theta_1}+\lambda = 0 \Rightarrow \theta_1 = -\frac{m_1}{\lambda} \\
\displaystyle \frac{m_2}{\theta_2}+\lambda = 0 \Rightarrow \theta_2 = -\frac{m_2}{\lambda}\\
\vdots \\
\displaystyle \frac{m_K}{\theta_K}+\lambda = 0 \Rightarrow \theta_K = -\frac{m_K}{\lambda}
\end{array}$  
$\because \theta_1+\theta_2+\cdots+\theta_K=1$  
$\displaystyle \therefore -\frac{m_1+m_2+\cdots+m_K}{\lambda}=1 \Rightarrow \lambda = -(m_1+m_2+\cdots+m_K)$，将该结果带入前面公式  
$\therefore \begin{array}{c} 
\displaystyle \theta_1 = -\frac{m_1}{\lambda} = \frac{m_1}{N} \\
\displaystyle \theta_2 = -\frac{m_2}{\lambda} = \frac{m_2}{N} \\
\vdots \\
\displaystyle \theta_K = -\frac{m_K}{\lambda} = \frac{m_K}{N}
\end{array}$  

## 贝叶斯估计  
&emsp;&emsp;又回顾掷硬币的例子，结果为$y \in \{正,反\}$，$\theta$的先验概率分布是B（贝塔）分布，正面出现的概率为$\theta_1$，反面出现的概率为$\theta_2$，$\displaystyle p(\theta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_1^{\alpha-1}\theta_2^{\beta-1}$  
&emsp;&emsp;如果$y \in \{c_1,\cdots,c_K\}$，对应的概率为$\theta_1,\cdots,\theta_K$，$\theta$的先验概率分布是Dirichlet分布，$\displaystyle p(\theta)=\frac{\Gamma(\alpha_1+\cdots+\alpha_K)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\theta_1^{\alpha-1}\theta_2^{\alpha-2}\cdots\theta_K^{\alpha-K}$  
&emsp;&emsp;在Beta分布中，当$\alpha=\beta>1$时，$\displaystyle \theta_1=\frac{1}{2}$的概率最大，在先验信息中，认为正面和反面出现的概率，很大的可能是相等的。同样在Dirichlet分布中，$\alpha_1=\alpha_2=\cdots=\alpha_K=\alpha$，也满足如上假设。  
&emsp;&emsp;获得样本之后，求取$\theta$的后验概率分布  
$$\begin{aligned} p(\theta|y_1,\cdots,y_N)
&= \frac{p(\theta,y_1,\cdots,y_N)}{p(y_1,\cdots,y_N)} \\
&\propto p(\theta)p(y_1,\cdots,y_N | \theta) \\
&\propto \theta_1^{\alpha-1}\theta_2^{\alpha-1}\cdots\theta_K^{\alpha-1}\cdot\theta_1^{m_1}\theta_2^{m_2}\cdots\theta_K^{m_K} \\
&\propto \theta_1^{m_1+\alpha-1}\theta_2^{m_2+\alpha-1}\cdots\theta_K^{m_K+\alpha-1}
\end{aligned}$$
&emsp;&emsp;上述是一个Dirichlet分布，为什么会是这个分布呢？正如看到$e^{-\theta^2+a\theta+b}$这种形式，是正态分布的感觉是一样的。  
根据上面的公式，可以写出系数：
$\displaystyle \frac{\Gamma(m_1+m_2+\cdots+m_K+K\alpha)}{\Gamma(m_1+\alpha)\Gamma(m_2+\alpha)\cdots\Gamma(m_K+\alpha)}$  
$\displaystyle \therefore p(\theta|y_1,\cdots,y_N)=\frac{\Gamma(m_1+m_2+\cdots+m_K+K\alpha)}{\Gamma(m_1+\alpha)\Gamma(m_2+\alpha)\cdots\Gamma(m_K+\alpha)}\theta_1^{m_1+\alpha-1}\theta_2^{m_2+\alpha-1}\cdots\theta_K^{m_K+\alpha-1}$  
&emsp;&emsp;由于计算贝叶斯估计是计算后验概率最大值，不需要如此精确的值，所以计算的时候去掉前面的系数，即$\max \theta_1^{m_1+\alpha-1}\theta_2^{m_2+\alpha-1}\cdots\theta_K^{m_K+\alpha-1}$  
最大后验结果为：  
$$\begin{aligned}\theta_1 
&= \frac{m_1+\alpha-1}{m_1+m_2+\cdots+m_K+K(\alpha-1)} \\
&= \frac{m_1+\alpha-1}{N+K\alpha-K}
\end{aligned}$$
易得：$\displaystyle \theta_2=\frac{m_2+\alpha-1}{N+K\alpha-K}$，后面省略。  
所以对应书上第51页中：$\lambda=\alpha-1$  

## 总结
&emsp;&emsp;这一部分所讲的内容，属于书上的扩展内容，比较超纲，如果没有看懂，也没有关系，可以尝试地推导一下，如果学有余力，可以学习扩展内容：LDA(Latent Dirichlet Alocation)模型，这个模型的基础就是Dirichlet分布和贝叶斯框架，感兴趣的同学可以查一下相关资料。