# 第7章-支持向量机-导读{docsify-ignore-all}
&emsp;&emsp;首先回顾一下感知机模型，感知机模型是当数据线性可分时，如何用一个超平面区分两类不同的数据。对于上述情况，支持向量机和感知机是非常相似的，它们的差别只在于决策函数（损失函数）的不同。  
&emsp;&emsp;7.1节介绍的就是线性可分的情况，会和感知机比较，当线性可分时，这就是一个最简单的情况，在7.2节会介绍如何处理数据线性不可分的情况。当一个线性模型，对数据的分类效果不好时，但数据可以用一个曲面进行分隔，将数据的输入变量做变换（即输入空间和特征空间不是完全一致的），在7.3节中，可以用核函数的方法。

## 线性可分支持向量机与硬间隔最大化
假设空间：$w \cdot x+b=0$  
决策函数：$f(x) = \text{sign}(w^* \cdot x + b^*)$，输出的类别是$\{+1,-1\}$。
<br/><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="image/7-1-Support-Vector.png"><br><div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #000;padding: 2px;">图7-1 支持向量</div></center>  

&emsp;&emsp;如图7-1所示，圆点和$\times$点为两种不同的类别，在感知机模型中，只要找到一个超平面把这两组数据分开就可以，得到的超平面不是唯一的，在SVM模型中，得到的超平面是唯一的，模型选择的依据就是**硬间隔最大化**，这个是什么意思呢？当选择一个超平面将两组数据分开，在二维问题中，这个超平面是一条直线（如图7-1中的实线），每一个点到该直线都有一个距离，使得每个点到直线的距离都比较大，因此这条直线就是唯一的，所有这些距离中最小的距离的点组成的平面称为**支撑超平面**（如图7-1的虚线），这些点被称为**支持向量**，图中实线为**分离超平面**，两个支撑超平面之间的距离称为**硬间隔**。“硬”表示所有的点不能在支撑超平面中间，**硬间隔最大化**就是支持向量机模型选择的标准。  
&emsp;&emsp;如何使得硬间隔最大化？书中讲述了两个概念：函数间隔和几何间隔。假设有一个超平面$w \cdot x + b =0$，某一个实例为$x^*$，函数间隔为$|w \cdot x^* + b|$，几何间隔为$\displaystyle \frac{|w \cdot x^* + b|}{\|w\|}$。  
&emsp;&emsp;书中描述要使得硬间隔最大化，使用几何间隔，而不用函数间隔。一个点到不同超平面的距离，不能用函数间隔表示。  
$\because |w \cdot x + b|$可以转换为$y_i(w \cdot x_i + b)$  
$\therefore$ 几何间隔可以转化为$\displaystyle \frac{y_i(w \cdot x_i + b)}{\|w\|}$  
$\because $ 硬间隔最大化表示点到超平面距离最小的点的距离最大  
$\therefore $ **硬间隔最大化公式**为$$\displaystyle \max_{w,b} \min_i \frac{y_i(w \cdot x_i + b)}{\|w\|}$$  
将$\displaystyle \frac{1}{\|w\|}$提取出来，变为$$\max_{w,b} \frac{1}{\|w\|} \min_i y_i(w \cdot x_i + b) $$  
令$y_i(w \cdot x_i + b) \geqslant 1$，可以转化为$$\begin{array}{ll}
\displaystyle \max_{w,b} & \displaystyle \frac{1}{\|w\|}\\ 
\text { s.t. } & y_i(w \cdot x_i + b) \geqslant 1
\end{array}$$  
$\because $ 最大化$\displaystyle \frac{1}{\|w\|}$和最小化$\displaystyle \frac{1}{2} \|w\|^2$是等价的  
最后线性可分的支持向量机的最优化问题为$$\begin{array}{ll}
\displaystyle \min_{w,b} & \displaystyle \frac{1}{2} \|w\|^2 \\ 
\text { s.t. } & y_i(w \cdot x_i + b) - 1 \geqslant 0
\end{array}$$  

对偶问题：$$\begin{array}{ll}
\displaystyle {\min_\alpha} & {\displaystyle \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i=1}^N \alpha_i} \\
\text { s.t. } & {\displaystyle \sum_{i=1}^N \alpha_i y_i = 0} \\
& {\alpha_i \geqslant 0,\quad i = 1,2, \cdots, N}
\end{array}$$  
最优解：$$w^*=\sum_{i=1}^N \alpha_i^* y_i x_i \\ b^* = y_j - \sum_{i=1}^N \alpha_i^* y_i(x_i \cdot x_j)$$  

## 线性支持向量机与软间隔最大化
### 支持向量
最优化问题：  
$$
\begin{array}{ll}
{\displaystyle \min _{w, b, \xi}} & {\displaystyle \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}} \\ 
\text { s.t. } & {y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N} \\ 
& {\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N}
\end{array}$$
<br/><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="image/7-2-Soft-Margin-Support-Vector.png"><br><div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #000;padding: 2px;">图7-2 软间隔的支持向量</div></center>  

&emsp;&emsp;软间隔最大化就是允许数据点出现在两个支撑超平面之间（如图7-2），加入了惩罚项，对误分类点进行惩罚，公式中$\displaystyle \frac{1}{2}\|w\|^2$表示两个支撑超平面之间的几何间隔尽可能大，会纳入更多的数据点，其中也会有误分类的数据点，数据点越多，惩罚$\displaystyle C \sum_{i=1}^{N} \xi_{i}$就越大，$C$为了衡量支撑超平面之间的间隔。当$C$取$\infty$时，该问题与7.1节的问题是一样的。

### 合页损失函数
最优化问题：$$
\sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$  
<br/><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="image/7-3-Hinge-Loss-Function.png"><br><div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #000;padding: 2px;">图7-3 合页损失函数</div></center>  

&emsp;&emsp;图7-3中，横轴表示函数间隔，函数间隔$ > 1- \xi_i$，$\xi_i$表示为惩罚，$\xi_i \geqslant 0$，当函数间隔$ \geqslant 1$时，$\xi_i = 0$（即没有惩罚），当函数间隔$ < 1$，$\xi = 1 - g(w \cdot x + b)$，在图中表示为实线，其函数为$[1 - y_i(w \cdot x_i + b)]_+$  

对偶问题：  
$$
\begin{array}{ll}
{\displaystyle \min _{\alpha}} & {\displaystyle \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j )-\sum_{i=1}^N \alpha_i} \\ 
{\text { s.t. }} & {\displaystyle \sum_{i=1}^N \alpha_i y_i=0} \\ 
{} & {0 \leqslant \alpha_i \leqslant C, \quad i=1,2, \cdots, N}
\end{array}
$$  
参数的最优解：
$$w^*=\sum_{i=1}^N \alpha_i^* y_i x_i \\ 
b^*=y_j-\sum_{i=1}^N y_i \alpha_i^*(x_i \cdot x_j)$$  

## 非线性支持向量机与核函数
<br/><center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="image/7-4-Nonlinear-Classification-Problems-and-Examples-of-Kernel-Skill.png"><br><div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #000;padding: 2px;">图7-4 非线性分类问题与核技巧示例</div></center>

&emsp;&emsp;7-4左图中，$x^{(1)}$和$x^{(2)}$是二维输入变量，分为$\bullet$点和$\times$点两类，没有办法用一条直线很好的分隔数据，从图中可以看到，$\times$点离原点近，$\bullet$点离原点远，将坐标进行变换$z=\phi(x),\phi(x)=x^2$，故$z^{(1)}={(x^{(1)})}^2, z^{(2)}={(x^{(2)})}^2$，最后可以得到7-4右图。可见右图是线性可分的，可以用超平面进行分隔，用新的$z$进行支持向量机，因为在支持向量机的对偶形式中出现的是内积形式，现在得到的内积为$k(x_i,x_j)=\phi(x_i) \cdot \phi(x_j)$。
  
对偶问题：$$
\begin{array}{ll}
{\displaystyle \min _{\alpha}} & {\displaystyle \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j)-\sum_{i=1}^N \alpha_i} \\ 
{\text { s.t. }} & {\displaystyle \sum_{i=1}^N \alpha_i y_i=0} \\ 
{} & {0 \leqslant \alpha_i \leqslant C, \quad i=1,2, \cdots, N}
\end{array}
$$
决策函数：$$
f(x)=\operatorname{sign}\left(\sum_{i=1}^N \alpha_i^* y_i K(x \cdot x_i)+b^{*}\right)
$$
原始形式的最优化问题：$$\begin{array}{ll} 
\min & \displaystyle \frac{1}{2}\|w\|^2 + C \sum_i \xi \\
\text{ s.t. } & y_i [w \cdot \phi(x)] \geqslant 1 - \xi_i \\
& \xi_i \geqslant 0
\end{array}$$  
分离超平面为$w \cdot \phi(x) + b = 0$  
**缺点：** 不知道用什么样的曲面能更好地分隔数据，选择核函数，并没有很强的依据，只能依靠经验去试验，也就是说用核函数技巧，可以解决曲面分类的一些问题，但同时用什么样的曲面去解决，这也是一个新的问题。  

## 序列最小最优化算法
对偶问题：$$
\begin{array}{ll}
{\displaystyle \min _{\alpha}} & {\displaystyle \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j)-\sum_{i=1}^N \alpha_i} \\ 
{\text { s.t. }} & {\displaystyle \sum_{i=1}^N \alpha_i y_i=0} \\ 
{} & {0 \leqslant \alpha_i \leqslant C, \quad i=1,2, \cdots, N}
\end{array}
$$  
&emsp;&emsp;需要优化的变量是$\alpha_i$有$N$个，当数据量很大的时候，需要优化的变量非常多，很难计算，所以不优化所有的变量，每一次优化其中的一部分变量。在该算法中，每次优化两个变量。